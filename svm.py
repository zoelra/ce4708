from cvxopt import matrix, solvers
from math import exp
import matplotlib.pyplot as plt
import numpy as np
from numpy import arange, array

training_data = np.loadtxt('training_data.txt', skiprows=1)
training_input, training_output = training_data[:, :2], training_data[:, 2]
testing_data = np.loadtxt('testing_data.txt', skiprows=1)
testing_input, testing_output = testing_data[:, :2], testing_data[:, 2]


# rbf_kernel(x,y,s2) -- Radial basis function kernel exp(-||x-y||^2/2*sigma^2).
# further represented as k
# Arguments:
#
# x,y  -- n-element vectors.
#
# s2   -- Variance of the R.B.F. kernel.
#         Squared standard deviation, sigma^2.
#
# Returns: Scalar value of kernel for the 2 input vectors.

def rbf_kernel(v1, v2, sigma2=0.25):
    assert len(v1) == len(v2)
    assert sigma2 >= 0.0
    mag2 = sum(map(lambda x, y: (x - y) * (x - y), v1, v2))
    return exp(-mag2 / (2.0 * sigma2))


# input_training     -- A list of vectors (lists) representing
#                       the input training patterns for this
#                       problem.
#
#                       e.g., [[0,0],[0,1],[1,0],[1,1]], the set
#                       of binary training patterns.
#
# output             -- A list of desired outputs.  These must
#                       be values in the set {-1,+1}.  If there
#                       are n input vectors in x, there must be
#                       exactly n values in t.
#
#                       e.g., [-1,-1,-1,1] -- the outputs
#                       for a 2-input AND function.
#
# k                  -- A Kernel function.
# c                  -- The softness variable

def make_lambdas(input_training, output, k=rbf_kernel, c=1000000.0):
    """Solve constrained maximaization problem and return list of l's."""
    p = make_p(input_training, output, k)
    n = len(output)
    q = matrix(-1.0, (n, 1))
    h = matrix(0.0, (2 * n, 1))
    for i in range(n):
        h[i + n] = c

    g = matrix(0.0, (2 * n, n))
    for i in range(2 * n):
        for j in range(n):
            if i < n:
                if j == i:
                    g[i, j] = -1.0
            if i >= n:
                if j == (i - n):
                    g[i, j] = 1.0

    a = matrix(output, (1, n), tc='d')
    r = solvers.qp(p, q, g, h, a, matrix(0.0))
    # print(r)

    output = [round(l, 6) for l in list(r['x'])]
    return (r['status'], output)


# make_b              -- Given the set of training vectors, "Xs", the set of
#                       training responses "Ts", and the set of Lagrange
#                       multipliers for the problem "Ls", return the bias for
#                       the classifier.
#
# Arguments:
#
# input_training     -- Inputs, as "input_training" in makeLambdas.
#
# output             -- A list of desired outputs.  As "output" in
#                       makeLambdas.
#
# lang_list          -- A list of Lagrange multipliers, the
#                       solution to the constrained optimaztion
#                       of W(L) as returned by a call to
#                       makeLambdas.  N.B., if this argument is
#                       None (the default), this routine will call
#                       generateLambdas automatically.
#
# k                  -- A Kernel function.

def make_b(input_training, output, lang_list=None, k=rbf_kernel, c=1000000.0):
    """Generate the bias given input_traing, Output and (optionally) Ls and K"""

    lang_list, dummy_b = set_multipliers_and_bias(input_training, output, lang_list, 0.0, k, c)

    sv_count = 0
    b_sum = 0.0
    for n in range(len(output)):
        if lang_list[n] >= 1e-10:
            sv_count += 1
            b_sum += output[n]
            for i in range(len(output)):
                if lang_list[i] >= 1e-10:
                    b_sum -= lang_list[i] * output[i] * k(input_training[i], input_training[n])

    return b_sum / sv_count


# classify          -- Classify an input vector using the Lagrange
#                       multipliers for the problem, the set of training
#                       inputs, "Xs", the set of desired outputs, "Ts" and
#                       bias "b", classify a vector "x".
#
# Arguments:
#
# x                 -- An input vector to classify (a list of values).
#
# input_training    -- A list of the training input vectors (hence a list
#                       of n-element lists).
#
# output            -- A list of desired outputs.  As "Ts" in
#                       makeLambdas.
#
# lang_list         -- A list of Lagrange multipliers.
#
# b                 -- The classifier bias, as generated by makeB.
#
# k                 -- A Kernel function.
#
# verbose           -- Controls whether or not the routine
#                       prints details about the current classification to
#                       the terminal as well as returning a status
#                       value.  Defaults to True.

def classify(x, input_training, output, lang_list=None, b=None, k=rbf_kernel, verbose=True, c=1000000.0):
    """Classify an input x into {-1,+1} given support vectors, outputs and L."""
    lang_list, b = set_multipliers_and_bias(input_training, output, lang_list, b, k, c)
    y = activation(x, input_training, output, lang_list, b, k)
    if verbose:
        print("{} {:8.5f}  -->".format(x, y), end=' ')
        if y > 0.0:
            print("+1")
        elif y < 0.0:
            print("-1")
        else:
            print("0  (ERROR)")
    if y > 0.0:
        return +1
    elif y < 0.0:
        return -1
    else:
        return 0


# testClassifier(input_training,output,lang_list,b,K,verbose)
#                   --Test a classifier by checking to see if its response
#                     to every training input Xs[i] is the desired output
#                     Ts[i].
#
# Arguments:
#
# input_training     -- A list of vectors (lists) representing
#                       the input training patterns for this
#                       problem.
#
# output             -- A list of desired outputs.  These must
#                       be values in the set {-1,+1}.

# lang_list          -- A list of Lagrange multipliers.
#
# b                  -- The classifier bias, as generated by makeB.
#
# k                  -- A Kernel function.
#
# verbose            -- Controls whether or not the routine
#                       prints details of misclassifications to the
#                       terminal as well as returning a status
#                       value.  Defaults to True.

# Note: If good is false test failed


def test_classifier(input_training, output_training, lang_list=None, b=None, k=rbf_kernel, verbose=True, do_test=False,
                    c=1000000.0, input_testing=None, output_testing=None):
    """Test a classifier specifed by Lagrange mults, bias and kernel on all Xs/Ts pairs."""
    assert len(input_training) == len(output_training)
    lang_list, b = set_multipliers_and_bias(input_training, output_training, lang_list, b, k, c)
    good = True
    train_mis_class = 0
    test_mis_class = 0
    for i in range(len(input_training)):
        train_c = classify(input_training[i], input_training, output_training, lang_list, b, k, verbose, c)
        if train_c != output_training[i]:
            if verbose:
                print("Misclassification: input {}, output {:d}, "
                      "expected {:d}".format(input_training[i], train_c, output_training[i]))
            train_mis_class += 1
    if train_mis_class >= int(len(output_training) / 2.0):
        good = False

    if good and do_test:
        for i in range(len(input_testing)):
            test_c = classify(input_testing[i], input_training, output_training, lang_list, b, k, verbose, c)
            if test_c != output_testing[i]:
                if verbose:
                    print("Misclassificaion: input {}, output {:d},"
                          "expected {:d}".format(input_testing, test_c, output_testing[i]))
                test_mis_class += 1
        if test_mis_class >= int(len(output_testing) / 2.0):
            good = False
    return good, train_mis_class, test_mis_class


# plotContours(input_training,output,lang_list,b,K,labelContours,labelPoints)
#                    -- Plot the contours of the decision boundary and
#                       +ve/-ve margins of a trained nonlinear SVM.  Also plots
#                       points from the training set (x,y) and t.
#
#                       N.B. Must be a 2-d classification problem (clearly
#                       can't plot contours of a higher-dimensional problem).
#
#
# Arguments:
#
# input_training     -- A list of vectors (lists) representing
#                       the input training patterns for this
#                       problem.
#
#                       e.g., [[0,0],[0,1],[1,0],[1,1]], the set
#                       of binary training patterns.
#
# output             -- A list of desired outputs.  These must
#                       be values in the set {-1,+1}.  If there
#                       are n input vectors in x, there must be
#                       exactly n values in t.
#
#                       e.g., [-1,-1,-1,1] -- the outputs
#                       for a 2-input AND function.
#
# lang_list          -- A list of Lagrange multipliers.
#
# b                  -- The classifier bias, as generated by makeB.
#
#                   -- A Kernel function.
#
# label_contours      -- Controls whether or not the contour
#                       lines for the decision boundary and margins are
#                       labelled or not.  If False (the default), they
#                       are not.  If set to the string 'auto', labels
#                       are automatically applied (which can result
#                       in visually unappealing plots, as the labels
#                       can end up hhidden behind data points).  If
#                       set to string 'manual', this flag tells the
#                       routine to allow the user to place contour
#                       labels interactively.
#
# labelPoints       -- Controls whether or not the routine
#                       plots the (x,y) and expected values of training
#                       points. Defaults to False.

def plot_contours(input_training, output, lang_list=None, b=None, k=rbf_kernel, label_contours=False,
                  label_points=False,
                  min_range=-0.6, max_range=1.6, step=0.05, c=1000000.0, plot_new_data=False, new_x=None, new_t=None):
    """Plot contours of activation function for a 2-d classifier, e.g. 2-input XOR."""
    assert len(input_training) == len(output)
    assert len(input_training[0]) == 2
    lang_list, b = set_multipliers_and_bias(input_training, output, lang_list, b, k, c)
    xs = arange(min_range, max_range + step / 2.0, step)
    ys = arange(min_range, max_range + step / 2.0, step)
    als = array([[activation([y, x], input_training, output, lang_list, b, k) for y in ys] for x in xs])
    cs = plt.contour(xs, ys, als, levels=(-1.0, 0.0, 1.0), linewidths=(1, 2, 1), colors=('blue', '#40e040', 'red'))
    if plot_new_data:
        for i, t in enumerate(new_t):
            if t < 0:
                col = 'blue'
            else:
                col = 'red'
            if label_points:
                plt.text(new_x[i][0] + 0.1, new_x[i][1], "%s: %d" % (new_x[i], t), color=col)
            plt.plot([new_x[i][0]], [new_x[i][1]], marker='o', color=col)
    else:
        for i, t in enumerate(output):
            if t < 0:
                col = 'blue'
            else:
                col = 'red'
            if label_points:
                plt.text(input_training[i][0] + 0.1, input_training[i][1], "%s: %d" % (input_training[i], t), color=col)
            plt.plot([input_training[i][0]], [input_training[i][1]], marker='o', color=col)

    if label_contours == 'manual':
        plt.clabel(cs, fontsize=9, manual=True)
    elif label_contours == 'auto':
        plt.clabel(cs, fontsize=9)
    plt.show()


# make_p -- Generates the P matrix for a kernel SVM problem.  See the
#               comments associated with generateLambdas below for a
#               discussion of the form and role of the P matrix.

def make_p(xs, ts, k):
    """Make the P matrix given the list of training vectors, desired outputs and kernel."""
    n = len(xs)
    assert n == len(ts)
    p = matrix(0.0, (n, n), tc='d')
    for i in range(n):
        for j in range(n):
            p[i, j] = ts[i] * ts[j] * k(xs[i], xs[j])
    return p


# def activation  -- Calculate the "activation level" of the classifier in
#                    response to a given input.  This is the input to the
#                    step nonlinearity that is the classifier output.
#
#                         y = b + sum_i Ls[i]*Ts[i]*K(Xs[i],X)
#
#                    for nonzero Lagrange multipliers.
#                    Used by routines "classify" and "plotContours".
#
# Arguments:
#
# x  -- Input point ot be classified (a list).
# input_training -- List of training points (list of lists).
# output -- List of training (i.e. desired) outputs.
# lang_list -- List of Lagrange multipliers for problem.
# b  -- Bias for problem.
# k -- Kernel function.


def activation(x, input_training, output, lang_list, b, k):
    """Return activation level of a point X = [x1,x2,....] given
       training vectors, training (i.e., desired) outputs, Lagrange
       multipliers, bias and kernel."""
    y = b
    for i in range(len(output)):
        if lang_list[i] >= 1e-10:
            y += lang_list[i] * output[i] * k(input_training[i], x)
    return y


# set_multipliers_and_bias -- A utility to set up the Lagrange
#                    multipliers and bias for a problem if they are not
#                    already established in the calling routine.
#
# Arguments:
#
# input_training        -- List of training points (list of lists).
# output        -- List of training (i.e. desired) outputs.
# lang_list -- List of Lagrange multipliers for problem.  If this
#              is None on the call, multipliers will be generated.
# b         -- Bias for problem.  If this is None on the call, a
#              bias will be generated.
# k         -- Kernel function.

def set_multipliers_and_bias(input_training, output, lang_list=None, b=None, k=rbf_kernel, c=1000000.0):
    if lang_list is None:
        status, lang_list = make_lambdas(input_training, output, k, c)
        if status != "optimal": raise Exception("Can't find Lambdas")
        # print("Lagrange multipliers:", lang_list)
    if b is None:
        b = make_b(input_training, output, lang_list, k, c)
        # print("Bias:", b)
    return lang_list, b


# soft margin
k = rbf_kernel
title = ''
contours = True
min_range = -5
max_range = 5
step = 0.05
c = 1.0

status, lang_list = make_lambdas(training_input, training_output, k, c)

"""Run a test problem."""

if status == "optimal":
    b = make_b(training_input, training_output, lang_list, k, c)
    # print("  bias:", b)
    Passed, softmargin_training_missed, softmargin_testing_missed = test_classifier(training_input, training_output,
                                                                                    lang_list, b, k, c=c, verbose=False,
                                                                                    do_test=True,
                                                                                    input_testing=testing_input,
                                                                                    output_testing=testing_output)
    if Passed:
        print("  Check PASSED")
        if contours:
            if title:
                t = title
            else:
                t = ""
            plt.figure(t, figsize=(6, 6))
            plot_contours(training_input, training_output, lang_list, b, k, False, False, min_range, max_range, step)
            plot_contours(training_input, training_output, lang_list, b, k, False, False, min_range, max_range, step,
                          plot_new_data=True, new_x=testing_input, new_t=testing_output)
    else:
        print("  Check FAILED: Classifier does not work correctly on training inputs for a soft margin")
    print("\n\n")

print("Soft margin training missclassification " + str(softmargin_training_missed))
print("Soft margin testing missclassification " + str(softmargin_testing_missed))
print("\n\n")

# hard margin

k = rbf_kernel
title = ''
contours = True
minRange = -5
maxRange = 5
step = 0.05
c = 1000000000.0

status, lang_list = make_lambdas(training_input, training_output, k, c)

"""Run a test problem."""

if status == "optimal":
    b = make_b(training_input, training_output, lang_list, k, c)
    # print("  bias:", b)
    Passed, hardmargin_training_missed, hardmargin_testing_missed = test_classifier(training_input, training_output,
                                                                                    lang_list, b, k, verbose=False, c=c,
                                                                                    do_test=True,
                                                                                    input_testing=testing_input,
                                                                                    output_testing=testing_output)
    if Passed:
        print("  Check PASSED")
        if contours:
            if title:
                t = title
            else:
                t = ""
            plt.figure(t, figsize=(6, 6))
            plot_contours(training_input, training_output, lang_list, b, k, False, False, min_range, max_range, step)
            plot_contours(training_input, training_output, lang_list, b, k, False, False, min_range, max_range, step,
                          plot_new_data=True, new_x=testing_input, new_t=testing_output)
    else:
        print("  Check FAILED: Classifier does not work correctly on training inputs for a soft margin")
    print("\n\n")
print("Hard margin training data missclassification " + str(hardmargin_training_missed))
print("Hard margin testing data missclassification " + str(hardmargin_testing_missed))
